{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c841780",
   "metadata": {},
   "source": [
    "## What this script does:\n",
    "\n",
    "1. Run Google Cloud Speech-to-Text to get transcription of wav files, which is constrained by existing human(-corrected) transcriptions\n",
    "2. Output a dictionary file that maps filenames onto transcriptions\n",
    "3. Read Chronset output and create a dictionary that maps filenames onto onset data\n",
    "4. Combine the transcription, onset, and context info to generate TextGrid files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb075093",
   "metadata": {},
   "source": [
    "## Data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d890b",
   "metadata": {},
   "source": [
    "The wav files must:\n",
    "- have a single channel\n",
    "- be named as {xxx}\\_{item_id}{condition}\\_{subject_id}.wav (e.g. exp_11a_dfsa.wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf36829",
   "metadata": {},
   "source": [
    "The keyword data must be in a dictionary where keys are the item IDs and the corresponding values are lists of keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768160e3",
   "metadata": {},
   "source": [
    "The context data (optional) must be in a dictionary where keys are the item IDs and the values are strings of contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e8745",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3422946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import textgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73f5661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory with wav files\n",
    "wav_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc24201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of wav files\n",
    "filenames = [i for i in os.listdir(wav_dir) if (\".wav\" in i and \"exp\" in i)]\n",
    "bnames = [os.path.splitext(f)[0] for f in filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182082c4",
   "metadata": {},
   "source": [
    "## Get candidate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92aaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest file from /automatic_transcription/keywords/ directory\n",
    "keyword_list = pickle.load( open( \"\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703602e",
   "metadata": {},
   "source": [
    "## Google Cloud Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a652d301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09cf3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the appropriate path to the API key\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5050c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps a file name onto a transcription for later use\n",
    "dict_gcloud = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6735f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to save the file names of items that the transcriber wasn't able to recognize\n",
    "failed_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb37f8",
   "metadata": {},
   "source": [
    "Based on a snippet from:\n",
    "https://cloud.google.com/speech-to-text/docs/sync-recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f00d5e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_2a_ckjd.wav:\thaunted\n",
      "exp_1b_arle.wav:\tfollowed\n",
      "exp_2b_bbxk.wav:\tspoken\n",
      "exp_1a_ahcw.wav:\tinterviewed\n",
      "exp_2a_bxxd.wav:\thaunted\n",
      "exp_1a_cbmp.wav:\tinterviewed\n",
      "exp_1a_ewki.wav:\tseen\n",
      "exp_2a_fncd.wav:\thaunted\n",
      "exp_1a_bcof.wav:\tfound\n",
      "exp_2b_bcof.wav:\tblade\n",
      "exp_1b_fncd.wav:\tmet\n",
      "exp_2b_ewki.wav:\tbecome\n",
      "exp_2b_cbmp.wav:\tseen\n",
      "exp_2b_ahcw.wav:\tseen\n",
      "exp_1b_bxxd.wav:\tliked\n",
      "exp_1a_bbxk.wav:\tinterviewed\n",
      "exp_2a_arle.wav:\thaunted\n",
      "exp_1b_ckjd.wav:\ttalked\n",
      "exp_2a_cnis.wav:\tvisited\n",
      "exp_1b_cnis.wav:\tspoken\n"
     ]
    }
   ],
   "source": [
    "# google cloud api\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "for file in filenames:\n",
    "    transc_list = []\n",
    "    \n",
    "    # Load the audio file\n",
    "    filepath = os.path.join(wav_dir, file)\n",
    "    with io.open(filepath, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    \n",
    "    # Load the key words\n",
    "    item_id = int(os.path.splitext(file)[0].split(\"_\")[1][:-1])\n",
    "    speech_context = speech.SpeechContext(phrases=keyword_list[item_id])\n",
    "    \n",
    "\n",
    "    # Transcription by Google Cloud Speech-to-Text\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        language_code=\"en-US\",\n",
    "        speech_contexts=[speech_context],\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for result in response.results:\n",
    "            transc_list.append(result.alternatives[0].transcript)\n",
    "        transc = \" \".join(transc_list)\n",
    "        \n",
    "        # If the transcription is empty\n",
    "        if len(transc) == 0:\n",
    "            transc = \"NOT_RECOGNIZED\"\n",
    "            failed_list.append(file)\n",
    "            \n",
    "        print(file+\":\\t\" + transc)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        print(\"Error at \", file)\n",
    "        failed_list.append(file)\n",
    "        transc = \"FAILED \"\n",
    "        \n",
    "    dict_gcloud[os.path.splitext(file)[0]] = transc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8834864b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e77dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Catch exceptionally short ones which might cause errors later\n",
    "short = [(f, dict_gcloud[f]) for f in dict_gcloud.keys() if len(dict_gcloud[f]) < 2]\n",
    "print(short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2459f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_gcloud, open( \"\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d4119",
   "metadata": {},
   "source": [
    "# Outputting files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16519cd",
   "metadata": {},
   "source": [
    "## Output functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7a14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates .TextGrid files for Praat\n",
    "# You can optionally pass a dictionary for the onset and/or for the duration of the files\n",
    "# The onset function is basically for Chronset\n",
    "# You can also give it context info to help hand correction\n",
    "\n",
    "# If the audio files have heterogeneous durations, add an argument of \"inconsistent = 1\"\n",
    "\n",
    "# The function replaces the RT with 0.1 if the RT is 0 or shorter, or is NaN, producing an error message\n",
    "def generate_textgrid(dictionary, output_dir, template, onset = dict(), context = dict(), inconsistent = 0):\n",
    "    import copy\n",
    "    \n",
    "    for k in dictionary.keys():\n",
    "        \n",
    "        try:\n",
    "            # Load the TextGrid template and edit it\n",
    "            tg = copy.deepcopy(template)\n",
    "            \n",
    "            # If the duration of the audio files are inconsistent, edit each TextGrid files to match the duration of the audio files\n",
    "            if inconsistent == 1:                     \n",
    "                audio = AudioSegment.from_file(os.path.join(wav_dir, k + \".wav\"))\n",
    "                tg[\"words\"][1].xmax = tg[\"words\"][-1].xmin = audio.duration_seconds - 0.001\n",
    "                \n",
    "                if tg.xmax != audio.duration_seconds:\n",
    "                    tg.xmax = audio.duration_seconds\n",
    "\n",
    "                    for tier in tg.keys():\n",
    "                        tg[tier][-1].xmax = audio.duration_seconds\n",
    "            \n",
    "            \n",
    "            tg[\"words\"][1].text = dictionary[k]\n",
    "\n",
    "            if k in onset.keys():\n",
    "                # If the onset is not detected or is negative, set 0.1\n",
    "                if np.isnan(onset[k]) or onset[k] <= 0 :\n",
    "                    print(\"No onset for \", k)\n",
    "                    tg[\"words\"][0].xmax = tg[\"words\"][1].xmin = 0.1\n",
    "                else:\n",
    "                    tg[\"words\"][0].xmax = tg[\"words\"][1].xmin = onset[k]\n",
    "\n",
    "            item_num = int(k.split(\"_\")[1][:-1])\n",
    "\n",
    "            if item_num in context.keys():\n",
    "                tg[\"context\"][0].text = context[item_num]\n",
    "\n",
    "            tg.write(os.path.join(output_dir, k + \".TextGrid\"))\n",
    "            \n",
    "        except:\n",
    "            print(\"Error at \", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4c445",
   "metadata": {},
   "source": [
    "## Setting up for outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde55a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_c = \"\" \n",
    "dict_gcloud = pickle.load(open(\"\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb3798",
   "metadata": {},
   "source": [
    "## Generate .textgrid files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cac7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a template\n",
    "template = textgrids.TextGrid(\"./temp_note.TextGrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0bc37a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File type = \"ooTextFile\"\n",
       "Object class = \"TextGrid\"\n",
       "\n",
       "xmin = 0.0\n",
       "xmax = 3.36\n",
       "tiers? <exists>\n",
       "size = 3\n",
       "item []:\n",
       "    item [1]:\n",
       "        class = \"IntervalTier\"\n",
       "        name = \"words\"\n",
       "        xmin = 0.0\n",
       "        xmax = 3.36\n",
       "        intervals: size = 3\n",
       "            intervals [1]:\n",
       "                xmin = 0.0\n",
       "                xmax = 0.5695189325640101\n",
       "                text = \"\"\n",
       "            intervals [2]:\n",
       "                xmin = 0.5695189325640101\n",
       "                xmax = 2.9325394879192213\n",
       "                text = \"==TRANSCRIPTION==\"\n",
       "            intervals [3]:\n",
       "                xmin = 2.9325394879192213\n",
       "                xmax = 3.36\n",
       "                text = \"\"\n",
       "    item [1]:\n",
       "        class = \"IntervalTier\"\n",
       "        name = \"context\"\n",
       "        xmin = 0.0\n",
       "        xmax = 3.36\n",
       "        intervals: size = 1\n",
       "            intervals [1]:\n",
       "                xmin = 0.0\n",
       "                xmax = 3.36\n",
       "                text = \"\"\n",
       "    item [1]:\n",
       "        class = \"IntervalTier\"\n",
       "        name = \"notes\"\n",
       "        xmin = 0.0\n",
       "        xmax = 3.36\n",
       "        intervals: size = 1\n",
       "            intervals [1]:\n",
       "                xmin = 0.0\n",
       "                xmax = 3.36\n",
       "                text = \"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c469ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the duration of an audio file and edit the template\n",
    "instance = AudioSegment.from_file(os.path.join(wav_dir, filenames[0]))\n",
    "template[\"words\"][1].xmax = template[\"words\"][-1].xmin = instance.duration_seconds - 0.001\n",
    "\n",
    "# Make sure the duration of the audio files matches the template\n",
    "\n",
    "if template.xmax != instance.duration_seconds:\n",
    "    template.xmax = instance.duration_seconds\n",
    "    \n",
    "    for k in template.keys():\n",
    "        template[k][-1].xmax = instance.duration_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59589071",
   "metadata": {},
   "source": [
    "### Get onset data from Chronset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4c5e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cfc279",
   "metadata": {},
   "source": [
    "#### Read the output of Chronset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98a4a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with Chronset output files\n",
    "ch_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64273539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load the output of Chronset\n",
    "# Create a dictionary that maps the file names onto onset time (sec)\n",
    "# Note that chronset uses miliseconds while praat uses seconds\n",
    "for textf in [t for t in os.listdir(ch_dir) if \".txt\" in t]:\n",
    "    onset_df = pd.read_table(os.path.join(ch_dir, textf), names = [\"wav\", \"rt\"])\n",
    "    temp_dict = dict(zip([os.path.splitext(w)[0] for w in onset_df[\"wav\"]], onset_df[\"rt\"] / 1000))\n",
    "    onset_dict.update(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7aa3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with context information\n",
    "cont_dict = pickle.load( open( \"\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "403d9c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No onset for  exp_2a_ckjd\n"
     ]
    }
   ],
   "source": [
    "# Generate textgrids with Google Cloud transcription\n",
    "# Omit the last argument if you don't use the onset info\n",
    "# set inconsistent = 1 if the durations of audio files are inconsistent\n",
    "\n",
    "generate_textgrid(dict_gcloud, output_dir_c, template, onset = onset_dict, context = cont_dict) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717fef2",
   "metadata": {},
   "source": [
    "**Recommended**  \n",
    "Run common_errors.ipynb after this to correct some common issues regarding homophones/inflections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
